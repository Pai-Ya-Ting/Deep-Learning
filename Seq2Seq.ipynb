{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Seq2Seq.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Pai-Ya-Ting/Deep-Learning/blob/main/Seq2Seq.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gesZ_Ws1Rc5"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\\\n",
        "df = pd.read_csv('train.csv',  dtype = {'file': str, 'scenario': str, 'sentence':str})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jQxWcuA7w1hP"
      },
      "source": [
        "import librosa\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext.legacy import data   \n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wxnrq-fntkMC"
      },
      "source": [
        "def read_audio(file_id, mode):\n",
        "    if mode == 'train':\n",
        "        filename = os.path.join(os.path.abspath('train/')+str('/' + file_id.file)+'.wav')\n",
        "    elif mode == 'test':\n",
        "        filename = os.path.join(os.path.abspath('test/')+'/'+str(file_id).zfill(6)+'.wav')\n",
        "    print(filename)\n",
        "    y, sr = librosa.load(filename)\n",
        "    if 0 < len(y): \n",
        "        y, _ = librosa.effects.trim(y)\n",
        "        y = librosa.util.fix_length(y, int(5*22050))\n",
        "        mfcc = librosa.feature.mfcc(y=y, n_mfcc = 40)\n",
        "    return mfcc\n",
        "\n",
        "for mode in ['train', 'test']:\n",
        "    X = []\n",
        "    cnt = 0\n",
        "    \n",
        "    if mode == 'train':\n",
        "        for i in df.itertuples():\n",
        "            x = read_audio(i, mode)\n",
        "            X.append(x)\n",
        "        np.save('MFCC_40_x_train', X)\n",
        "        print(np.array(X).shape)\n",
        "        \n",
        "    elif mode == 'test':\n",
        "        for i in range(4721):\n",
        "            x = read_audio(i, mode)\n",
        "            X.append(x)\n",
        "        np.save('MFCC_40_x_test', X)\n",
        "        print(np.array(X).shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jPTWH0VYtkME"
      },
      "source": [
        "def build_dict():\n",
        "    category.build_vocab(train_data)\n",
        "    title.build_vocab(train_data)#, unk_init=torch.Tensor.normal_)\n",
        "    \n",
        "    vocab = title.vocab\n",
        "    vocab_size = len(title.vocab)\n",
        "    n_class = len(category.vocab)\n",
        "    \n",
        "    PAD_IDX= vocab.stoi[title.pad_token]\n",
        "    UNK_IDX = vocab.stoi[title.unk_token]\n",
        "    \n",
        "    return vocab_size, vocab"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p7YXmdkmtkMF"
      },
      "source": [
        "category = data.Field(batch_first=True, pad_token=None, unk_token=None) \n",
        "stop_words = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
        "title = data.Field(fix_length=30, stop_words=stop_words, \n",
        "                   batch_first=True, lower= True, include_lengths=True, init_token='<sos>', eos_token='<eos>')\n",
        "\n",
        "train_data = data.TabularDataset(\n",
        "   path = './train.csv',\n",
        "   format = 'csv',\n",
        "   fields = [(None, None), ('category', category), ('title', title)],\n",
        "   skip_header = True\n",
        ")\n",
        "\n",
        "vocab_size, vocab = build_dict()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dWfOStzptkqc"
      },
      "source": [
        "# df = pd.read_csv('./train.csv')\n",
        "df_sentence = []\n",
        "for i in range(df.shape[0]):\n",
        "  s = df['sentence'][i].split()\n",
        "  s = [\"<sos>\"] + s + [\"<eos>\"] + [\"<pad>\"]*(30 - len(s))\n",
        "  df_sentence.append(s)\n",
        "  #  break\n",
        "df_sentence = np.array(df_sentence)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HD9wstUM9jjK"
      },
      "source": [
        "y = []\n",
        "max_len = 30\n",
        "for i in df_sentence:\n",
        "  s = list(map(lambda x: vocab.stoi[x],i))\n",
        "  s = s + [vocab.stoi[\"<PAD>\"]]*(max_len - len(s))\n",
        "  y.append(s)\n",
        "  # print(s)\n",
        "y = np.array(y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxuThusu_1p7"
      },
      "source": [
        "X = np.load('MFCC_40_x_train.npy', allow_pickle=True)\n",
        "# y = np.load('./sample_rate_22050/y.npy', allow_pickle=True).astype(np.float32)\n",
        "label = np.load('label.npy', allow_pickle=True).astype(np.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-NBamDr4u4_d",
        "outputId": "f75d9bf8-53aa-4d72-b075-26f4401a1a1a"
      },
      "source": [
        "X.shape, y.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((18052, 40, 216), (18052, 32))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjShmtAp5AiN"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X, y, test_size=0.33, random_state=42, stratify = label)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yjryOwLueKoc"
      },
      "source": [
        "X_train, y_train = torch.from_numpy(X_train), torch.from_numpy(y_train)\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "X_val, y_val = torch.from_numpy(X_val), torch.from_numpy(y_val)\n",
        "val_dataset = TensorDataset(X_val, y_val)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BcSCmcCuD8bJ"
      },
      "source": [
        "train_loader = DataLoader(train_dataset, batch_size = 128)\n",
        "val_loader = DataLoader(val_dataset, batch_size = 128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UWL3lMETqkVj"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Encoder, self).__init__()\n",
        "\n",
        "        self.hidden_size = 40\n",
        "        self.dim = 40\n",
        "        self.rnn = nn.LSTM(self.dim, self.hidden_size, 5, batch_first=True, bidirectional=True)\n",
        "        \n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "    def forward(self, outputs, hidden = None):\n",
        "        outputs = outputs.to(device)\n",
        "        outputs = self.dropout(outputs)\n",
        "        outputs, hidden = self.rnn(outputs, hidden)\n",
        "        ln = nn.LayerNorm(outputs.size()[1:])\n",
        "        output = ln(outputs)\n",
        "        \n",
        "        return self.dropout(outputs), hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odM5IYCGqlO9"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Decoder, self).__init__()\n",
        "\n",
        "        self.max_length = 20\n",
        "        self.hidden_size = 40\n",
        "        self.dim = 40\n",
        "#         self.output_size = 256\n",
        "\n",
        "        self.rnn = nn.LSTM(self.dim, self.hidden_size, 5, batch_first=True, bidirectional=True)\n",
        "        self.embedding = nn.Embedding(vocab_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2 * 2, self.hidden_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "        self.out = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "\n",
        "    def forward(self, outputs, hidden, enc_outputs):\n",
        "        outputs = self.embedding(outputs.long())\n",
        "        outputs = self.dropout(outputs)\n",
        "        outputs, hidden = self.rnn(outputs[0], hidden)\n",
        "        \n",
        "        attn_weights = torch.sum(outputs*enc_outputs, dim=2)#.transpose(1, 0)\n",
        "        context = torch.bmm(attn_weights.unsqueeze(1), enc_outputs)\n",
        "        cats = self.attn_combine(torch.cat((outputs, context), dim=2))\n",
        "#         pred = F.log_softmax(self.out(cats.tanh().squeeze(0)), dim=1)\n",
        "        pred = self.out(cats.tanh().squeeze(0))\n",
        "        ln = nn.LayerNorm(pred.size()[1:])\n",
        "        pred = ln(pred)\n",
        "        \n",
        "        return pred, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L-qnjLKFs-TQ"
      },
      "source": [
        "import random\n",
        "class Seq2Seq(nn.Module):\n",
        "    def __init__(self, MAX_LENGTH=10, learning_rate=0.001):\n",
        "        super(Seq2Seq, self).__init__()\n",
        "        self.encoder = Encoder().to(device)\n",
        "        self.decoder = Decoder().to(device)\n",
        "\n",
        "        self.encoder_optimizer = optim.AdamW(self.encoder.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
        "        self.decoder_optimizer = optim.AdamW(self.decoder.parameters(), lr=learning_rate, weight_decay=1e-3)\n",
        "#         self.criterion = nn.CTCLoss(blank=2, reduction='mean') #nn.CrossEntropyLoss()\n",
        "        self.criterion = nn.MSELoss()\n",
        "#     nn.CrossEntropyLoss(ignore_index=2)\n",
        "\n",
        "    def forward(self, source, target, teacher_forcing_ratio=0.5, MAX_LENGTH=20):\n",
        "        input_length = source.size(1)\n",
        "        target_length = target.size(1)\n",
        "\n",
        "        self.encoder_optimizer.zero_grad()\n",
        "        self.decoder_optimizer.zero_grad()\n",
        "        encoder_outputs = torch.zeros(source.size(0), max(input_length, MAX_LENGTH), 2 * self.encoder.hidden_size, device=device)#, requires_grad=True)\n",
        "\n",
        "        for i in range(input_length):\n",
        "            if i == 0:\n",
        "                encoder_output, encoder_hidden = self.encoder(source[:,[i],:])\n",
        "            else:\n",
        "                encoder_output, encoder_hidden = self.encoder(source[:,[i],:], encoder_hidden)\n",
        "#             print(input_length, encoder_outputs[:,i].shape, encoder_output[:,0].shape)\n",
        "            encoder_outputs[:,i] = encoder_output[:,0]\n",
        "        \n",
        "        # encoder hidden -> decoder hidden\n",
        "        decoder_hidden = encoder_hidden#.to(device)\n",
        "        decoder_input = torch.zeros(1, source.size(0), 1, device = device)\n",
        "        loss = 0\n",
        "        \n",
        "        record = np.zeros(128, dtype=np.bool)\n",
        "        out = torch.zeros(source.size(0), target_length, device = device)\n",
        "        for i in range(target_length):\n",
        "            decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "            teacher_forcing = (random.random() < teacher_forcing_ratio)\n",
        "            out[:, [i]] = self.find_word(decoder_output).float()#.numpy()\n",
        "    \n",
        "            decoder_input = (target[:,[i]] if teacher_forcing else out[:,[i]].detach())\n",
        "#             print(target[:,[i]].shape)\n",
        "            target_em = self.decoder.embedding(target[:,[i]])\n",
        "            loss += self.criterion(decoder_output, target_em)\n",
        "            \n",
        "#             print(out[:10, i], target[:10, i])\n",
        "            \n",
        "            record[torch.where(decoder_input.cpu().squeeze() == 1)] = True\n",
        "            if record.all():\n",
        "                break\n",
        "            decoder_input = decoder_input.reshape(1,-1,1)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        self.encoder_optimizer.step()\n",
        "        self.decoder_optimizer.step()\n",
        "\n",
        "        return (loss.item() / target_length)\n",
        "    \n",
        "    def find_word(self, sample):\n",
        "        distance = torch.norm(self.decoder.embedding.weight.data - sample, dim=1)\n",
        "        nearest = torch.argmin(distance, dim=1).unsqueeze(1)\n",
        "        return nearest\n",
        "\n",
        "    def train(self, n_epoch):\n",
        "        for iter in range(1, n_epoch + 1):    \n",
        "            for step, (data, label) in enumerate(train_loader):\n",
        "                data = torch.transpose(data,1,2)\n",
        "                loss = model(data, label)\n",
        "            print(f'Epoch {iter} loss: {loss/len(train_loader)}')\n",
        "    \n",
        "    def evaluate(self, n_epoch):\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for iter in range(1, n_epoch + 1):\n",
        "                for step, (data, label) in enumerate(test_loader):\n",
        "                    data = torch.transpose(data,1,2)\n",
        "                    loss = model(data, label)\n",
        "                    epoch_loss += batch_loss.item()\n",
        "                print(f'Evaluate {iter} loss: {epoch_loss / len(data)}')\n",
        "#                 epoch_acc += batch_acc.item()\n",
        "#             return epoch_loss / len(data), epoch_acc / len(data)\n",
        "\n",
        "    def predict(self, source):\n",
        "        epoch_loss = 0\n",
        "        epoch_acc = 0\n",
        "\n",
        "        model.eval()\n",
        "        ans = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            \n",
        "            target_length = 30\n",
        "#             for source in data: \n",
        "            input_length = source.size(1)\n",
        "            encoder_outputs = torch.zeros(source.size(0), max(input_length, 20), 2*self.encoder.hidden_size, device=device)#, requires_grad=True)\n",
        "            out = torch.zeros(source.size(0), target_length, device = device)\n",
        "            \n",
        "            for i in range(input_length):\n",
        "                if i == 0:\n",
        "                    encoder_output, encoder_hidden = self.encoder(source[:,[i],:])\n",
        "                else:\n",
        "                    encoder_output, encoder_hidden = self.encoder(source[:,[i],:], encoder_hidden)\n",
        "                encoder_outputs[:,i] = encoder_output[:,0]\n",
        "\n",
        "            decoder_hidden = encoder_hidden#.to(device)\n",
        "            decoder_input = torch.zeros(1, source.size(0), 1, device = device)\n",
        "            record = np.zeros(source.size(0), dtype=np.bool)\n",
        "\n",
        "            for i in range(target_length):\n",
        "                decoder_output, decoder_hidden = self.decoder(decoder_input, decoder_hidden, encoder_outputs)\n",
        "#                 print(decoder_output)\n",
        "                out[:, [i]] = self.find_word(decoder_output).float()#.numpy()\n",
        "\n",
        "                decoder_input = out[:, [i]].detach()\n",
        "                record[torch.where(decoder_input.cpu().squeeze() == 1)] = True\n",
        "                if record.all():\n",
        "                    break\n",
        "                decoder_input = decoder_input.reshape(1,-1,1)\n",
        "#                 print(out[:, [i]])\n",
        "#                 print(out[:, [i]].numpy().shape)\n",
        "                if i == 0:\n",
        "                    ans = out[:, [i]].numpy()\n",
        "                else:\n",
        "                    ans = np.concatenate((ans, out[:, [i]].numpy()), axis=1)\n",
        "\n",
        "        return ans"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jd_0ZzAsqJ9M"
      },
      "source": [
        "n_epoch = 20\n",
        "model = Seq2Seq().to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "OWds1FVJtkMS",
        "outputId": "0a48707c-16a5-486c-ca3c-a45ff7ab6511"
      },
      "source": [
        "model.train(n_epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 loss: 0.004541007154866269\n",
            "Epoch 2 loss: 0.004026145370383012\n",
            "Epoch 3 loss: 0.0033144442658675343\n",
            "Epoch 4 loss: 0.002636147800244783\n",
            "Epoch 5 loss: 0.00223097236532914\n",
            "Epoch 6 loss: 0.0019250065088272095\n",
            "Epoch 7 loss: 0.00175166318291112\n",
            "Epoch 8 loss: 0.001670705801562259\n",
            "Epoch 9 loss: 0.0014947892803894847\n",
            "Epoch 10 loss: 0.0014445830332605462\n",
            "Epoch 11 loss: 0.0013085918206917613\n",
            "Epoch 12 loss: 0.0012229103006814656\n",
            "Epoch 13 loss: 0.001137005106398934\n",
            "Epoch 14 loss: 0.0011573891106404757\n",
            "Epoch 15 loss: 0.0010197523393129047\n",
            "Epoch 16 loss: 0.0009744279478725635\n",
            "Epoch 17 loss: 0.000914025698837481\n",
            "Epoch 18 loss: 0.0008442620697774385\n",
            "Epoch 19 loss: 0.0008205631061604148\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "hCuA4iUltkMU"
      },
      "source": [
        "dd = [[]]\n",
        "for i, (d, _) in enumerate(val_loader):\n",
        "    d = torch.transpose(d,1,2)\n",
        "    if i == 0:\n",
        "        dd = model.predict(d)\n",
        "    else:\n",
        "        dd = np.concatenate((dd,model.predict(d)), axis = 0)\n",
        "\n",
        "    for j in set(dd.flatten()):\n",
        "        print(i, vocab.itos[int(j)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RZ7XpST4tkMU"
      },
      "source": [
        "df_test = pd.DataFrame(index=range(len(X_val)), columns = ['title'])\n",
        "for i in range(len(X_val)):\n",
        "    s = []\n",
        "    for j in range(30):\n",
        "        s.append(vocab.itos[int(dd[i, j])])\n",
        "    df_test['title'][i] = str(' '.join(s))\n",
        "df_test.head(30)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Y51_2qztkMV"
      },
      "source": [
        "X_test = np.load('MFCC_40_x_test.npy', allow_pickle=True)\n",
        "tryloader = DataLoader(X_test, batch_size = len(X_test), shuffle = False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qohny_CItkMV"
      },
      "source": [
        "for d in tryloader:\n",
        "    d = torch.transpose(d,1,2)\n",
        "    dd = model.predict(d)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B2iMVKkotkMV"
      },
      "source": [
        "set(dd.flatten())\n",
        "for i in set(dd.flatten()):\n",
        "    print(i, vocab.itos[int(i)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kms0FXzXtkMW"
      },
      "source": [
        "dd.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "urvL2KsctkMW"
      },
      "source": [
        "df_test = pd.DataFrame(index=range(len(X_test)), columns = ['title'])\n",
        "for i in range(len(X_test)):\n",
        "    s = []\n",
        "    for j in range(30):\n",
        "        voc = vocab.itos[int(dd[i, j])]\n",
        "        if voc == '<sos>' or voc == '<pad>' or voc == '<eos>':\n",
        "            continue\n",
        "        s.append(vocab.itos[int(dd[i, j])])\n",
        "#         list(dict.fromkeys(s))\n",
        "    df_test['title'][i] = str(' '.join(s))\n",
        "df_test.head(100)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcE1c8C4tkMX"
      },
      "source": [
        "df_test.to_csv('df_test.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4o-MepUItkMX"
      },
      "source": [
        "# Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOa-VumitkMX"
      },
      "source": [
        "import torch   \n",
        "import pandas as pd\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchtext.legacy import data\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "SEED = 2021\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "torch.cuda.manual_seed(SEED)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def accuracy(y_pred, y_test):\n",
        "    pred = torch.argmax(y_pred, dim = 1, keepdim = True).squeeze(1)\n",
        "    return (pred == y_test).sum()/len(y_test)\n",
        "\n",
        "def train_cls(data, optimizer_cls, criterion_cls):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model_cls.train()\n",
        "    for batch in data:\n",
        "        optimizer_cls.zero_grad()\n",
        "        \n",
        "        text, text_len = batch.title\n",
        "        \n",
        "        prediction = model_cls(text)#.squeeze(1)\n",
        "#         print(prediction, batch.category)\n",
        "        batch_loss = criterion_cls(prediction, batch.category.squeeze(1))\n",
        "        batch_acc = accuracy(prediction, batch.category.squeeze(1))\n",
        "\n",
        "        batch_loss.backward()\n",
        "        nn.utils.clip_grad_norm_(model_cls.parameters(), 0.01)\n",
        "        optimizer_cls.step()\n",
        "\n",
        "        epoch_loss += batch_loss.item()\n",
        "        epoch_acc += batch_acc.item()\n",
        "\n",
        "    return epoch_loss / len(data), epoch_acc / len(data)\n",
        "\n",
        "def predict_cls(data, model_cls):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "    model_cls.eval()\n",
        "    ans = []\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in data:\n",
        "            text, text_len = batch.title\n",
        "            prediction = model_cls(text)#.squeeze(1)\n",
        "            pred = torch.argmax(prediction, dim = 1, keepdim = True).squeeze(1)\n",
        "            ans.extend(list(map(lambda x: category.vocab.itos[x], pred)))\n",
        "            \n",
        "        return pd.DataFrame(ans, columns=['Category'])\n",
        "\n",
        "def evaluate_cls(data):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "    \n",
        "#     model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in data:\n",
        "            text, text_len = batch.title\n",
        "            prediction = model_cls(text)#.squeeze(1)\n",
        "\n",
        "            batch_loss = criterion_cls(prediction, batch.category.squeeze(1))\n",
        "            batch_acc = accuracy(prediction, batch.category.squeeze(1))\n",
        "            \n",
        "            epoch_loss += batch_loss.item()\n",
        "            epoch_acc += batch_acc.item()\n",
        "        return epoch_loss / len(data), epoch_acc / len(data)\n",
        "    \n",
        "def prepare_data(train, test):\n",
        "    train = data.BucketIterator(\n",
        "      (train),\n",
        "      sort_key = lambda x: len(x.title),\n",
        "      sort = True,\n",
        "      sort_within_batch=True,\n",
        "      batch_size = 128,\n",
        "#     shuffle=False,\n",
        "      device = device\n",
        "    )\n",
        "\n",
        "    test = data.BucketIterator(\n",
        "      (test),\n",
        "      batch_size = 128,\n",
        "        sort = False,\n",
        "        sort_within_batch=False,\n",
        "        shuffle=False,\n",
        "      device = device\n",
        "    )\n",
        "    \n",
        "    return train, test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axmPfXaKtkMY"
      },
      "source": [
        "class Transformerlayer(nn.Module):\n",
        "    def __init__(self):\n",
        "    \n",
        "        super(Transformerlayer, self).__init__()\n",
        "        dropout = 0.1\n",
        "        nheads = 4\n",
        "        dim_feedforward = 128\n",
        "        \n",
        "        self.embedding_dim = 64\n",
        "        self.attn = nn.MultiheadAttention(embed_dim = self.embedding_dim, dropout = dropout, num_heads = nheads)\n",
        "        self.linear1 = nn.Linear(self.embedding_dim, dim_feedforward)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.linear2 = nn.Linear(dim_feedforward, self.embedding_dim)\n",
        "        self.norm1 = nn.LayerNorm(self.embedding_dim)\n",
        "        self.norm2 = nn.LayerNorm(self.embedding_dim)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.dropout3 = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, x, src_mask, padding_mask):\n",
        "        x1, weights = self.attn(x, x, x, attn_mask=src_mask, key_padding_mask= padding_mask)\n",
        "        x = x + self.dropout2(x1)\n",
        "        x = self.norm1(x)\n",
        "        x1 = self.linear2(self.dropout1(F.relu(self.linear1(x))))\n",
        "        x = x + self.dropout3(x1)\n",
        "        x = self.norm2(x)\n",
        "        \n",
        "        return x, weights\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self):\n",
        "    \n",
        "        super(Transformer, self).__init__()\n",
        "\n",
        "        self.embedding_dim = 64\n",
        "        self.embedding = nn.Embedding(vocab_size, self.embedding_dim)\n",
        "        single_encoder = Transformerlayer()\n",
        "        self.Encoder = nn.ModuleList([single_encoder for i in range(2)])\n",
        "\n",
        "        self.Decoder = nn.Linear(self.embedding_dim, len(category.vocab))\n",
        "        self.pos = PositionalEncoding(self.embedding_dim)\n",
        "\n",
        "    def forward(self, text):\n",
        "        outputs = (self.embedding(text)) * np.sqrt(self.embedding_dim) # (batch_size, title_len, embedding_dim)\n",
        "        outputs = self.pos(outputs.transpose(0, 1)).to(device)  # (batch_size, title_len, embedding_dim)\n",
        "        padding_mask = (torch.zeros((outputs.shape[1], outputs.shape[0])) == vocab.stoi['<pad>']).to(device)\n",
        "        \n",
        "        weights = []\n",
        "        for m in self.Encoder:\n",
        "            outputs, weight = m(outputs, None, None)\n",
        "            weights.append(weight)\n",
        "\n",
        "        outputs = outputs.transpose(0, 1)\n",
        "        outputs = torch.mean(outputs, dim = 1)\n",
        "        outputs = F.softmax(self.Decoder(outputs), dim=1)\n",
        "\n",
        "        return outputs\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=30):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(max_len, dtype=torch.float).unsqueeze(1)\n",
        "        \n",
        "        term1, term2 = torch.arange(0, d_model, 2).float(), (- np.log(10000.0) / d_model)\n",
        "        div_term = torch.exp(term1*term2)\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.to(device)\n",
        "        x = x + (self.pe[:x.size(0), :]).to(device)\n",
        "        return self.dropout(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yr-5wMXKtkMY"
      },
      "source": [
        "N_EPOCHS = 25\n",
        "best_loss = float('inf')\n",
        "train_loss = train_acc = 0\n",
        "l_train, acc_train = [], []\n",
        "l_valid, acc_valid = [], []\n",
        "\n",
        "vocab_size, vocab = build_dict()\n",
        "model_cls = Transformer().to(device)\n",
        "\n",
        "optimizer_cls = optim.AdamW(model_cls.parameters(), lr=1e-3, betas=(0.9, 0.98), eps = 1e-9, weight_decay=1e-3)\n",
        "criterion_cls = nn.CrossEntropyLoss()#ignore_index = vocab.stoi['<pad>'])\n",
        "learning_rate = []\n",
        "\n",
        "train_set, valid_set = train_data.split(split_ratio=0.8, random_state=random.getstate())\n",
        "training_data, testing_data = prepare_data(train_set, valid_set)\n",
        "\n",
        "for epoch in range(N_EPOCHS):\n",
        "\n",
        "    train_loss, train_acc = train_cls(training_data, optimizer_cls, criterion_cls)\n",
        "    valid_loss, valid_acc = evaluate_cls(testing_data)\n",
        "    \n",
        "    if valid_loss < best_loss:\n",
        "        best_loss = valid_loss\n",
        "        best_model = model\n",
        "    \n",
        "    acc_train.append(train_acc)\n",
        "    l_train.append(train_loss)\n",
        "\n",
        "    acc_valid.append(valid_acc)\n",
        "    l_valid.append(valid_loss)\n",
        "\n",
        "    learning_rate.append(optimizer_cls.param_groups[0]['lr'])\n",
        "\n",
        "    # if (epoch+1) % 5 == 0:\n",
        "    print(f'Epoch: {epoch+1}')\n",
        "    print('learning rate: ', optimizer_cls.param_groups[0]['lr'])\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\tValid Loss: {valid_loss:.3f} | Valid Acc: {valid_acc*100:.2f}%')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHI5I1JQtkMZ"
      },
      "source": [
        "plt.plot(l_train)\n",
        "plt.plot(l_valid)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clwheoAdtkMZ"
      },
      "source": [
        "plt.plot(acc_train)\n",
        "plt.plot(acc_valid)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aWzL6SQWtkMa"
      },
      "source": [
        "test_data = data.TabularDataset(\n",
        "   path = './df_test.csv',\n",
        "   format = 'csv',\n",
        "   fields = [('title', title)],\n",
        "   skip_header = True\n",
        ")\n",
        "\n",
        "training_data, testing_data = prepare_data(train_data, test_data)\n",
        "\n",
        "ans = predict_cls(testing_data, model_cls)\n",
        "ans.insert(0, column=\"File\", value = ans.index.values)\n",
        "ans.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3k9Y-FotkMa"
      },
      "source": [
        "ans.to_csv('ans1.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQ5szfHPtkMa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}